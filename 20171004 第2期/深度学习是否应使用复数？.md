# 深度学习是否应该使用复数?

大家不认为深度学习只能使用实数这很奇怪嘛？或许，深度学习使用复数才是更加奇怪的事情吧（注意：复数是有虚部的）。一个有价值的论点是：我们大脑在计算中不会使用复数。然而你也可以这样争论：大脑也不用矩阵运算或者链式法则微分啊。此外，人工神经网络（ANN）具有实际神经元的模型。长期以来，我们用实分析代替了生物合理性。深度学习的研究者发现线性代数和基本微积分足以展示开创性的结果，他们满足于此。

然而，为什么我们要止步于实分析呢？我们已经用了这么久线性代数和微分方程，那我们也可以将这一切都推倒，用复分析建立新的一套。或许更加奇妙的复分析会让我们找到更强大的方法。毕竟它对量子力学奏效，那么它也有可能在深度学习领域有所作为。此外，深度学习和量子力学都与信息处理有关，二者可能是同一件事情。

由于论据的原因，我们暂且不考虑生物合理性。这是一个很古老的观点，可以追溯到   1957   年   Frank   Rosenblatt   第一次提出人工神经网络的时候。那么问题出现了，复数可以提供哪些实数不能提供的东西呢？

在过去几年里，曾经出现过一些探索在深度学习中使用复数的文章。奇怪的是，它们中的大部分都没有被同行评议的期刊接受。因为深度学习的传统观念在该领域已经很流行了。但是，我们还是要讨论一些有趣的论文。

DeepMind   有一篇论文《Associative   Long   Short-Term   Memory》，文中探讨了使用复数值形成联想记忆神经网络。该系统被用来增强   LSTM   （长短期记忆网络）的记忆。论文的结论是使用复数的网络可获取更大的记忆容量。根据数学原理，与只使用实数的情况相比，使用复数需要的矩阵更小。如下图所示，使用复数的神经网络在内存开销上与传统   LSTM   有显明显不同。

Yoshua Bengio 和他的团队在Montreal探索了关于使用复数的另一方面。在一篇标题为“酉进化递归神经网络”（Martin Arjovsky,Amar Shah,Yoshua Bengio创造)的论文中，调查者探索了酉阵。他们认为如果矩阵的特征值接近1的话，那么在减少梯度的不扽或许会带来好处。在研究中，他们研究用复数作为RNN网络的权重。得到了这样的结论：

事实证明我们的uRNN能更好的通过长序列传递梯度信息，并且不会有像LSTM一样多的饱和隐藏状态。

他们做了多次实验去比较用复数的网络和传统 的RNN网络的性能：使用复数的网络明显比传统的有着更好、更稳定的表现。

FAIL和EPEL的团队有一篇类似的论文《Kronecker Recurrent Units》，他们在论文里在复制任务中用酉阵的可行性。他们展示了一种能够大幅减少所需参数的矩阵分解方法。论文中描述了他们使用复数的动机。 

因为实空间的行列式是连续函数，所以实空间的酉集是不连贯的。因而，使用标准的连续优化程序不能在实值网络上跨越全酉集。相反，酉集在复空间中是连接在一起的，因为它的行列式是复空间中单位圆上的点，所以使用复数的话就不会出现这个问题。

这篇论文的精华之一就是下面这个富有建设性的想法：

状态应当保持高维度，以此去使用高容量的网络将输入信息编码成内部状态、提取预测值。但   recurrent   dynamic  （递归动态） 可使用低容量模型实现。

目前，这些方法已经探索了在   RNN（循环神经网络）   上对复数值的使用。MILA（蒙特利尔学习算法研究所）最近的一篇论文《Deep   Complex   Networks》（Chiheb   Trabelsi   等人）进一步探索了这些方法在卷积神经网络上的使用。论文作者在计算机视觉任务上测试了他们的网络，结果很有竞争力。卷积神经网络的创造者yann LeCun也有一篇相关的论文叫《A mathematical motivation for complex-valued convolutional networks》，这篇文章论述了使用复数的合理性。

最后，我们必须说一下复数在   GAN  （生成式对抗网络） 中的使用。毕竟   GAN   可以说是最热的话题了。论文《Numerics   of   GANs》探讨了   GAN   中麻烦的收敛性能。他们研究了带有复数值的雅克比矩阵的特点，并使用它创建解决   GAN   均衡问题的最先进方法。

在去年的一篇博客中，我介绍了全息原理和深度学习的关系。文章中的方法探索了张量网络和深度学习架构网络之间的相似性。量子力学可以理解为是使用了一种更加通用的概率形式。复数的使用提供了常规概率无法提供的额外能力。具体来说就是叠加和干扰的能力。为了实现全息，在处理过程中使用复数会比较好。

在机器和深度学习空间中大量的数学分析倾向于使用贝叶斯思想作为观点。事实上，大多数从业者都认为它是贝叶斯的，但实际上它来自与统计学机制（除去名字，这里没有统计学的那些繁文缛节）。

但如果量子力学是广义的概率，那如果我们使用   QM   启发的方法替代会发生什么呢？一些论文试图研究这一方向，结果值得注意。在去年的一篇论文《Quantum   Clustering   and   Gaussian   Mixtures》中，作者探索了无监督硬聚类的使用情况。文章是这样说的：

因此，我们观察到了量子类干扰现象并不在高斯混合模型中出现。我们展示了量子方法在所有方面上都优于高斯混合方法。

下图为两者的对比：

噪声发生了什么？

那么我们要想想，为什么在有了   20   世纪的量子概率理论后还要拘泥于   18   世纪的贝叶斯理论呢？（注意：令人震惊的是统计学家的货物崇拜自十八实际起就开始了。）

或许复数没有被经常使用的原因是研究者对它不够熟悉。在优化研究社区中，数学传统并没有涉及到复数。然而物理学家却一直在使用复数。那些虚部在量子力学中始终是存在的。这并不奇怪，这就是现实。我们仍然不太理解为何这些深度学习系统会如此有用。所以探索其他的表示可能会带来出乎意料的突破。

想法的替代可能会带来一些意想不到的图片。这就是我们的现在，偶然发现AGI(通用人工智能)突破的队伍获得胜利。

在不久的将来，这个局面可能会变化。最先进的结构可能会普遍使用复数，到那时候不使用复数反倒变得奇怪了。我想到了那个时候，18世纪的贝叶斯理论会完全过时。

