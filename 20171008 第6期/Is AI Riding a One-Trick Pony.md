# AI是否一招鲜吃遍天？

原文链接：[Is AI Riding a One-Trick Pony?](https://www.technologyreview.com/s/608911/is-ai-riding-a-one-trick-pony/?from=hackcv&hmsr=hackcv.com&utm_medium=hackcv.com&utm_source=hackcv.com)

**几乎所有AI的进步都取决于三十年前的突破，而现在想要跟上项目的前进步伐需要突破AI的限制。**

我站在了即将成为世界中心的地方，不过也可能只是多伦多中心塔楼的7层的一个闪闪发亮的房间。 Jordan Jacobs带我参观了这附近，他是这个地方的创办者之一：新的矢量研究所，今年秋天开始正式营业，它的目标是成为全球人工智能的的中心。

我们将位置定在多伦多是因为“深度学习”之父 Geoffrey Hinton在多伦多。深度学习是当前AI火爆的技术。Jacobs说：“30年后，我们再回顾过去，说 Geoff是AI届的Einstein，深度学习，我们称之为AI的东西。”在深度学习研究顶尖的领域中，Hinton的引文比接下来的三篇文章加在一起还多。他的学生和博士后在OpenAI、Facebook、Apple的AI实验室工作。而他本人则是 Google Brain AI 团队的首席科学家。事实上，在过去的十年中，人工智能在翻译、语音识别、图像识别和应用于游戏方面的成就几乎都可以追溯到Hinton的工作。

 矢量研究所是Hinton思想兴起的纪念碑，是一个研究中心，来自美国和加拿大各地的公司，如谷歌、尤伯和恩维迪亚，将资金投入以促进人工智能技术商业化。资金涌入的速度比 Jacobs要求的要快；他的两位共同创始人调查了多伦多地区的公司，发现该地区对人工智能专家的需求是加拿大每年人工智能专家人数的10倍。从某种意义上说，矢量对于现在全世界围绕深度学习的努力者来说是零起点：兑现技术，教授技术，改进和应用技术。数据中心正在建设，塔楼内充满了创业公司，整整一代学生都进入了这个领域。

你站在向量工作室光秃秃的地板上，四周很安静，你的心里只有一个感觉，你正在做某些事情的开始。但是深度学习的独特之处在于它的关键思想的大小。Hinton与同事David Rumelhart 和 Ronald Williams的突破性论文在1986年发布了。论文中阐述了一种叫做反向传播的技术，简称为反向支撑技术。用普林斯顿大学计算心理学家Jon Cohen反向支撑技术是“所有深度学习的基础—几乎一切”。

也就是说，今天的人工之讷讷感是深度学习，深度学习是基础—而基础已经有了30多年的历史，这真是不可思议。所以这是值得去了解的，去了解到底发现了什么—使得一个技术可以持续使用那么长时间然后突然变强—因为一旦你了解了深度学习的故事，你就会明白人工智能的当前局势，你就会发现其特别之处在于我们可能已经不在革命的开始，而在革命的结束。

### 辩护

从矢量研究所走到Hintor在谷歌的工作室，Hintor的大部分时间都是在工作室里度过的（他现在室多伦多大学的名誉教授）。至少在夏天，是这个城市的一个活广告。你可以理解为什么Hinton，一个原籍英国的人，在匹兹堡的卡内基梅隆大学工作后，在1980年代搬到这里。

当你走出去的时候，即使实在金融区附近的市中心，你也会觉得自己身处于大自然。你会闻到一个气味，我想是空气中湿润的土壤的气息。多伦多建在森林覆盖的峡谷之上，据说是"花园城市"；由于它已经城市化，当地政府已经制定了严格的限制赖保护树木。当你坐飞机来这座城市时，你会发现这座城市外部看起来就像童话场景一样郁郁葱葱。

也许我们并不在一场革命的开始。

多伦多的北美的第四大城市（仅次于墨西哥、纽约、洛杉矶），这里十分的多元化：有一半以上的人口出生于加拿大以外。你看看周围走动的人群。技术走廊里的人群不像旧金山—年轻人的白人穿着国际化的衣服。这里有免费的医疗保障、很好的公立学校、人们十分友好、政治秩序相对落后和稳定。这些东西吸引了像Hinton这样的人，他们说他离开了美国是因为伊朗的逆反。这是我们在午饭前和他见面时谈论的第一件事。

”CMU的大多数人认为美国入侵Nicaragua是完全合理的“，他说。“他们认为Nicaragua就应该是属于他们的。”他告诉我他最近在一个项目上取得了重大突破：“得到了一位与我一起工作的优秀工程师。”一位名叫 Sara Sabour的女士。Sabour是伊朗人，她在美国的工作签证被拒绝了。谷歌的多伦多工作室吸引了她。

69岁的Hinton长得和蔼、消瘦、典型得英国人面容。嘴巴薄、耳朵大、鼻子高挺。他出生在英格兰的Wimbledon，声音听起来像一本儿童科学书的讲述者：好奇、迷人、渴望解释事物。他十分有趣，有点喜欢表演。他在我们的整个谈话过程中一直站着，因为事实证明，对他来说，坐着太痛苦了。“2005年6月，我坐了下来，然后结果弄错了。”他告诉我，在将他背后的圆盘解释清楚前让奇怪的线先落地给他带来了麻烦。这意味着他不能飞，那天的早些时候他不得不把一个看起来像冲浪板的装置带到牙医办公室，这样他就可以躺在上面检查牙根裂痕。

20世纪80年代， Hinton和现在一样，是神经网络专家，就是我们大脑神经元和突触网络的简化模型。然而，在那时，人们已经坚定地认为神经网络是AI研究的死角。尽管最早的神经网络，始于上世纪50年代的Perceptron，被誉为人类迈向机器智能的第一步，麻省理工的MarvinMinsky和SymourPapert在1969年出版的一本书，叫做Perceptrons，从数学上证明了这种网络可以运行、只有最基本的功能。这些网络只有两层神经元，一个输入层和一个输出层。在输入和输出神经元之间理论上具有更多层的网络可以解决各种各样的问题，但是没有人知道如何训练它们，因此在实践中它们是无用的。除了一些像Hinton这样的僵持者之外，Perceptrons导致大多数人完全放弃神经网络。

在1986年，Hinton取得了突破，他展示了反向传播可以训练深度神经网络，就是一个具有两到三层以上的神经网络。但这一直到了26年后，计算能力才得到了提升。Hinton和[他的学生](https://www.technologyreview.com/lists/innovators-under-35/2015/visionary/ilya-sutskever/)在2012年的[论文](https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf)中提出了使用反向传播技术训练的深度神经网络在图像识别方面打败了最先进的系统。“深度学习实现了质的飞跃“。对外界来说，AI似乎一夜直接火爆了。但对于Hinton来说，这是他付出了很多以后的回报。

### 现实扭曲力场

一个神经网络通常就像一个三明治一样构成，一层叠着一层。这些层就有人工神经元，他们是计算单元，他们兴奋—就像真正的神经元一样—将兴奋传递给他们相连接的其他神经元。可以用数字表示一个神经元的兴奋。比如0.13，32.39。数字说明的兴奋的程度。在每两个相连接的神经元中还有另一个关键的数字，它决定了兴奋应该传导多少给另一个。这个数字是为了模拟大脑中神经元之间突触的强度。这个数字越高，意味着连接越强，就会传递更大的兴奋程度。

Hinton、David Rumelhart和Ronald Williams关于“错误传播”的开创性著作。

深度神经网络最成功的应用之一就是图像识别，比如HBO在硅谷的那个可以判断图片中是否有热狗这样的项目。这样的项目真的存在，这是10年前不可能做到的。要让他们开始工作，第一步是获取一张图片。简单点说，它是一个有着100个像素宽和100个像素高的小黑白图片。你将图片放入神经网并用过设置输入层神经元的兴奋程度，使它等于每个像素的亮度。这就是三明治的底层：10000个神经元（100×100），代表图像中每个像素的亮度。

然后把这一大层神经元连接到另一大层神经元上，比如说几千个，然后这些又连接到几千个神经元的另一层，这样持续好多层。最后，在三明治的最上层，也就是输出层，你只有两个神经元，一个代表“热狗”，另一个代表“不是热狗”。现在的想法是教会神经网络如果图片中有热狗那么就只激发第一个神经元，如果图片中没有热狗就只激发第二个神经元。反向传播技术—Hinton已经建立了他的职业生涯—正是这样做的方法

虽然这个神经网络在处理大数据时的效果很好，但是它的后台支持非常简单。这就是为什么大数据在AI中那么重要—为什么Facebook和Google对它那么渴求，以及为什么矢量研究所决定与加拿大最大的四家医院发展成为数据伙伴关系。

这种情况下，数据采集数百万张图片，一些有热狗、一些没有；并将有热狗的进行标注。当你第一次创建你的神经网络时，神经之间的连接应该时任意权重的—随机数字表示每个连接要传递多谁刺激，就像大脑的突触还没有进化的状态。反向传播就是要改变这些权重，使他们正常工作。这样的结果是当你将有热狗的图片输入到神经网络中的输入层，输出层的“热狗”神经元就变得兴奋。

假设你的第一张训练照片是一张钢琴的图片。你把图片的像素强度的100x100为10000的数字，每一个都是神经元网络的底层。由于兴奋传播的网络根据相邻层神经元之间的连接强度，它最终会在最后一层，有两个神经元，说图片中是否有热狗。因为输入的图片是一架钢琴，最理想的是“热狗”神经元应该输出0就可以了，而不是“热狗”神经元应该输出一个高一些的数。但是让我们说，这种结果并不奏效。假设网络对这个图片的识别是错误的。反向传播是对于一个给定的训练例子程序，更改网络中的每一个连接的强度，修复错误的过程。

它的工作方式是，从最后两个神经元开始，找出它们的错误程度：兴奋值理论上应该是多少，它们实际上是多少？完成后，您将查看通向这些神经元的每个连接（下一层中的连接），并找出它们对错误的贡献。你一直这样做，直到你已经走到了第一组连接，在网络的最底层。此时，您知道每个单独的连接对总体错误贡献了多少，在最后一步中，你尽可能减少总体错误的去更改每个权重。这种技术称为“反向传播”，因为从输出开始，您正在通过网络“向后（或向下）传播”错误。

令人难以置信的是，当你使用数百万或数十亿张图像进行此操作后，网络开始非常擅长于判断图像中是否包含热狗。更值得注意的是，这些图像识别网络的各个层开始能够以与我们自己的视觉系统相同的方式“看到”图像。也就是说，第一层可能最终检测出边缘，也就是说，它的神经元在有边缘时兴奋，而在没有边缘时不兴奋；该层上面的一层可能能够检测出边缘集合，比如角落；该层上面的一层可能开始看到形状；再上一层，它可能会开始寻找像“open bun ”或“closed bun”之类的东西，在这个意义上，神经元对任何一种情况都有反应。换言之，网络将自己组织成层次结构，而不必经过明确的编程。

当你将问题进行一些小改变时，人工智能不会崩溃。

这是每个人都着迷的东西。这不仅意味着神经网络很好的分类热狗或别的东西的图片：它们似乎能够表达想法。通过文字，你可以更清楚地看到这一点。你可以把维基百科数十亿字长的文本，输入一个简单的神经网络，训练它，对于每一个单词，一个大的数字列表，对应于一个层中每个神经元的兴奋。如果你把这些数字看成是一个复杂空间中的坐标，那么你所做的就是在这个空间中的某个地方找到一个点。现在，训练你的网络，这样一来，在维基百科页面上出现彼此接近的词语就会得到相似的坐标，而且，天哪，有些疯狂的事情发生了：具有相似含义的词语开始在空间中彼此接近地出现。也就是说，“nsan”和“unhinged”将具有彼此接近的坐标，“three”和“seven”等等。更重要的是，所谓的向量算法使得可以从矢量“法国”中减去“巴黎”的矢量，添加“意大利”的矢量，最后在“罗马”附近，没有任何人明确地告诉网络罗马是意大利的，巴黎是属于法国的。

“这太神奇了，”Hinton说。“这太令人震惊了。”神经网络可以被认为是试图把所有事物——图像、文字、某人谈话的记录、医学数据——放入数学家所说的高维向量空间，在那里事物的距离或接近度反映了他真实的世界一些中重要的特征。Hinton相信这就是大脑本身所做的。“如果你想知道一个想法是什么，”他说，“我可以用一串词语来表达它。我可以说：“约翰想，”哎哟，“但是如果你问，‘什么是思想？”约翰有这样的想法意味着什么？他头脑里并没有开头引语，也没有“哎哟”和结尾引语，甚至还有一个清理过的版本。在他的脑袋里有一些神经活动的大模式。“如果你是个数学家，那么大的神经活动模式可以在向量空间中捕捉到，每个神经元的活动都对应一个数字，每个数字对应一个真正大向量的坐标。在Hinton看来，思想就是一种矢量的舞蹈。

![img](https://cdn.technologyreview.com/i/images/geoff-3342-cmyk-final-web_0.jpg?sw=600&cx=0&cy=0&cw=864&ch=1296)

多伦多的AI顶尖机构以矢量命名事实绝非巧合。Hinton就是那个名字矢量研究所的创始人。

Hinton创造了一种现实扭曲场，一种确定性和热情的氛围，让你感觉到没有矢量不能做的事情。毕竟，看看他们已经能够生产什么：自动驾驶汽车，检测癌症的计算机，能够即时翻译口语的机器。看看这位迷人的英国科学家谈论高维空间中的梯度下降的样子！

只有当你离开房间时，你才会想起来：这些“深度学习”系统仍然十分笨拙，尽管它们有时看起来多么聪明。他们在看到一堆油炸圈饼的图片堆放在桌子上，会自动地把它称为“一堆甜甜圈堆在桌子上”，似乎理解了世界，但是当它看到一个女孩刷牙的照片时说：“男孩拿着棒球棒。”你就会意识到，如果它真的存在的话，它的理解真的很差。

 神经网络只是没有思想的的模糊模式识别，正如模糊模式识别有用的原因，人们因此急于将它们集成到几乎每一种软件中使用，在最好的情况下，有限的品牌的智慧，是一个很容易被愚弄。深度神经网络识别的图像的功能很容易被影响当你改变一个像素，或增加视觉噪声是觉察不到的人。事实上，在我们发现新的方法来应用深度学习时，我们也发现了更多的局限性。无人驾驶的汽车无法在以前从未见过的条件下航行。机器在分析需要对日常生活有常识性了解的句子时遇到困难。 

在某些方面，深层学习模仿了人类大脑中正在发生的事情，但是仅仅以一种简单的方式——这也许解释了为什么它的智力有时看起来如此肤浅。的确，反向传播不是通过深入大脑，解码思想本身来发现的；它是由动物在古老的经典条件反射实验中通过反复试验来学习的模型发展而来的。当它发展起来的时候，其中大部分的进步并没有涉及一些关于神经科学的新见解；它们是由多年的数学和工程学所达到的技术进步。目前，我们所不知道的远远多于我们所知道的。

Hinton说，“大多数会议都包含一些细微的变化……而不是努力思考然后说，‘我们现在所做的真正不足的是什么？’有什么困难吗？让我们集中注意力在困难上。“

当你看到内部的一个又一个的巨大进步时，这是很难从外部欣赏。但现在AI的最新进展没有工程那么科学，甚至有时候是修修补补。尽管我们已经开始更好地掌握了什么样的变化可以改进深层学习系统，但我们仍然在很大程度上不了解这些系统是如何工作的，或许它们可以组成像人类思想这样强大的东西。

值得思考的是，关于反向传播，我们是否已经绞尽脑汁。如果是这样的话，这可能意味着人工智能进展的停滞期。

##### 给你的一些推荐

1. [New autonomous farm wants to produce food without human workers](https://www.technologyreview.com/s/612230/new-autonomous-farm-wants-to-produce-food-without-human-workers/)
2. [Quantum machine learning is a big leap away, at least for now](https://www.technologyreview.com/the-download/612235/quantum-machine-learning-is-a-big-leap-away-at-least-for-now/)
3. [The first “social network” of brains lets three people transmit thoughts to each other’s heads](https://www.technologyreview.com/s/612212/the-first-social-network-of-brains-lets-three-people-transmit-thoughts-to-each-others-heads/)
4. [How to know if you’re affected by Facebook’s massive data breach](https://www.technologyreview.com/the-download/612218/how-to-know-if-youre-affected-by-the-facebooks-massive-data-breach/)
5. [Jeff Bezos has just bagged a massive rocket engine deal](https://www.technologyreview.com/the-download/612214/jeff-bezos-has-just-bagged-a-massive-rocket-engine-deal/)

### 耐心

如果你想看到下一个大的成果，一些可以构成具有灵活智能的机器基础的东西，你应该去看看那些在80年代遇到反向传播的研究结果：聪明的人会放弃那些没有起到作用的想法。

几个月前，我去了总部设在麻省理工学院的“大脑与机器中心”，它是一个多伦多的机构。去那里看我的一个朋友，Eyal Dechter，并为他的认知科学论文辩护。就在演讲开始前，他的妻子 Amy,、他们的狗Ruby和他们的女儿苏珊娜在附近闲逛，祝他一切顺利。屏幕上是他的狗的照片，接下来一张是她女儿小时候的样子。当爸爸要求苏珊娜指出自己的时候，她高兴地拿了一个长的可缩回的指针来指着她自己小时候的照片。在走出房间的路上，她推着一辆玩具推车跟在她妈妈身后喊道：“祝你好运，爸爸！”在她的肩膀上。“Vámanos！“她最后说。她才两岁。

事实是它并不起作用只是暂时的烦恼。

Eyal以一个引人思考的问题开始他的谈话：Susannah,经历两年的时间，怎么能学会说话、玩耍、说故事呢？是什么使人类大脑学习得这么好？计算机能如此快速、如此流畅地学习吗？

我们从我们已经了解的事物中理解新现象。我们把一个领域分解成碎片，学习这些碎片。Eyal是一个数学家和程序员，他认为制作蛋奶酥这样的任务，就像制作复杂的计算机程序一样。但是并不是说你通过学习无数个程序的微指令，比如“转动手肘30度，然后俯视桌面，然后伸出你的手指，然后……”来学习制作蛋奶酥，如果你必须为每个新任务都这样做，那么学习就太难了。你会努力坚持你已经知道的。一样的来看，我们按照高级步骤来编写程序，比如“搅打蛋清”，它们本身由子程序组成，比如“敲打鸡蛋”和“分离蛋黄”。

相关的故事

![img](https://cdn.technologyreview.com/i/images/mj17-aiblackbox1.jpg?sw=180&cx=0&cy=377&cw=1563&ch=879)

AI核心的黑暗秘密。

 没有人真正知道最先进的算法是如何做到的。这是一个问题。 

计算机不这么做，这也是他们愚蠢的一个重要原因。要想让深度学习系统能够识别一个热狗，你必须给它喂4000万张热狗图片。为了让Susannah认出一只热狗，你给她看了一条热狗。用不了多久，她对语言的理解就会比意识到某些词经常出现在一起更深刻。与计算机不同，她的脑子里会有一个关于整个世界如何运作的概念。Eyal说：“对我来说，人们害怕计算机工作这是不可思议的一件事。就像计算机不能取代律师并不是因为律师确实做了复杂的事情，而是因为律师会阅读并会与人交谈。这并不是说我们不是很亲密，我们走得太远了。”

当你稍微改变它试图解决的问题的要求时，真正的智力不会出现问题。Eyal的论文的关键部分是他的演示，原则上，如何让计算机以这种方式工作：流畅地将计算机已经知道的应用到新的任务中，快速引导其从几乎不知道的新领域到成为专家的方式。

![img](https://cdn.technologyreview.com/i/images/nd17-hinton1.png?sw=600&cx=0&cy=0&cw=2235&ch=2365)

Hinton为他的下一个大创意画了这个草图，用“胶囊”来组织神经网络。

本质上，这是一个他称之为“探索-压缩”算法的过程。它让计算机像程序员一样工作，在构建越来越复杂的程序的过程中建立一个可重用、模块化的组件库。计算机不被告知任何有关新领域的知识，而是试图通过尝试、巩固所发现的内容以及尝试更多的东西来构建关于它的知识，就像人类孩子所做的那样。

他的导师Joshua Tenenbaum是AI届受高度重视的研究者之一。Tenenbaum的名字经常出现在我和其他科学家谈话的。DeepMind—AlphaGo背后的团队—在2016年围棋的游戏中击败了一位世界冠军选手，这让计算机科学家大吃一惊的一些关键人物都曾经担任过他的博士后。他参与了一家创业公司，试图给自动驾驶汽车提供一些关于基本物理学和其他司机意图的直觉，这样他们就能更好地预见在他们以前从未见过的情况会如何处理，比如当一辆卡车在他们面前行驶，或者当有人非常快的跑过去。

Eyal的论文还没有被翻译成那些实际的应用，更不用说那些号称任何能成为打败人类的头条新闻的程序了。Tenenbaum说，Eyal正在努力解决的问题“真的很难”。“这需要很多很多代人。”

Tenenbaum有一头长长的卷曲的美白头发，当我们坐下来喝咖啡时，他穿了一件带黑裤的纽扣衬衫。他告诉我，他关注的是反向传播的故事。几十年来，反向传播是一个很酷的数学，并没有真正完成任何事情。随着计算机变得越来越快，工程变得越来越复杂，它突然出现了。他希望同样的事情可能发生在他自己的作品和他的学生身上，“但这可能还要再花几十年。”

至于Hinton，他确信克服人工智能的局限性需要建立“计算机科学与生物学之间的桥梁”。在这个观点中，Backprop是生物学启发的计算的胜利；这个想法最初不是来自工程，而是来自心理学。现在，Hinton正试图采取类似的方法。

如今的神经网络是由大而平的层构成的，但是在人类新皮层中，真正的神经元不仅水平地排列成层，而且垂直地排列成列。Hinton认为他知道列是做什么用的，例如，即使我们的观点发生了变化，它们对于我们识别对象的能力也是至关重要的。所以他正在构建一个人工版本，他称之为“胶囊”来测试这个理论。到目前为止，它还没有被淘汰，胶囊并没有显著改善他的神经网络的性能。就像他在研究反向传播的那30年的情况一样。

“这件事一定是正确的，”他谈到胶囊理论时，嘲笑自己的大胆。“它现在不起作用只是暂时的烦恼。”

*James Somers*是一位作家和程序员，居住在纽约。在2015年5月/6月，他在《麻省理工技术评论》上发表的文章是的“思维的工具包”，文章展示了他们使用的编程语言是如何塑造互联网创业企业的。