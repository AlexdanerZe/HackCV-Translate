# 大脑vs深度学习的第一部分：计算复杂度（为什么奇点离我们还很远）

原文链接：[The Brain vs Deep Learning Part I: Computational Complexity — Or Why the Singularity Is Nowhere Near](http://timdettmers.com/2015/07/27/brain-vs-deep-learning-singularity/?from=hackcv&hmsr=hackcv.com&utm_medium=hackcv.com&utm_source=hackcv.com)

在这篇博客中，我将深入研究大脑，并解释其基本信息处理机制，并将其与深度学习进行比较。我通过一步一步地沿着大脑电化学和生物信息处理管道进行操作，并将其直接与卷积网络的架构相关联。因此，我们将看到神经元和卷积网络是非常相似的信息处理机器。在进行比较时，我还将讨论这些过程的计算复杂性，从而推导出对大脑总体计算能力的估计。我将使用这些估计，以及来自高性能计算的知识来表明，本世纪不太可能出现技术奇点。

这篇博客很复杂，因为它涉及多个主题，以便将它们统一为一个连贯的思想框架。我尽量使这篇文章具有可读性，但可能并没有在所有地方都成功。因此，如果你发现自己读到一个不明确的段落中，那么我可能会在接下来的几段中将其与另一门学科相结合，重新解释地更清楚。

首先，我将简要介绍技术奇点的预测和与之相关的主题。 然后我将开始整合大脑和深度学习之间的思想。 最后，我将讨论高性能计算以及这一切与预测技术奇点的关系。

将大脑信息处理步骤与深度学习相比较的部分是独立的，对技术奇点的预测不感兴趣的读者可以会跳过这一部分。

## 第一部分: 评估技术奇点的当前预测

最近有很多关于人工智能最早将在2030年达到超人类智能的头条预测新闻，这可能预示着人类灭绝的开始，或者至少对日常生活产生很大的影响。那么，这个预测是如何做出的？

### 有助于预测奇点的因素

Ray Kurzweil做了很多非常准确的[预测](https://en.wikipedia.org/wiki/Predictions_made_by_Ray_Kurzweil#2029)，他实现这些预测的方法对于计算设备来说非常简单:观察计算能力、效率和大小的指数增长，然后进行推断。通过这种方式，你可以很容易预测出适合掌上小型计算机的出现，只要有一点创造力，就可以想象有一天会出现平板电脑和智能手机。趋势已经出现了，你只需要想象之后掌上计算机可以用来做些什么。

雷·库兹韦尔(Ray Kurzweil)同样预言了强人工智能的出现，这种人工智能与人类一样聪明，甚至更聪明。在这个预测中，他还使用了计算能力指数增长的数据，并将其与大脑计算能力的估计进行了比较。

他还承认该软件与硬件一样重要，并且强人工智能软件的开发需要更长的时间，因为这种软件只有在快速计算机系统可用时才能开发。 这可以在深度学习领域感受到，由于计算机速度慢，上世纪90年代的坚实想法是不可行的。 一旦使用图形处理单元（GPU），这些计算限制很快就会被消除，并且可以快速取得进展。

然而，Kurzweil还强调，一旦达到硬件水平，第一个“简单”的强人工智能系统将很快地被开发。 他将类似大脑的计算能力的出现设定为2020年，强人工智能(第一种类似人类的智能或更好的智能)的出现时间设定为2030年。为什么这些数字？ 随着2019年计算能力的持续增长，我们将达到相当于人脑的计算能力 - 我们真的会吗？

这个估计基于两件事：（1）对大脑复杂性的估计，（2）对计算能力增长的估计。 正如我们将要看到的，这两种估计都不是最新的神经科学和高性能计算的技术和知识。

我们对神经科学的了解每年都会翻倍。用这个翻倍的时间，在2005年我们只掌握了我们今天所掌握的神经科学知识的0.098%。这个数字有点偏差，因为2005年的倍增时间约是2年，而现在还不到一年，但总体来说还是低于1%。

事实上，Ray Kurzweil根据他对2005年神经科学的预测，从未更新过它们。 基于1％的神经科学知识对大脑计算能力的估计似乎并不正确。 以下是过去两年里的几项重大发现，这些发现将大脑的计算能力提高了许多个数量级：

- 研究表明，大脑连接本身可以以有意义的方式处理信息和改变神经元的行为，而不是被动的连接，例如： 大脑连接可以帮助你在日常生活中看到物体。仅这一事实就将大脑的计算复杂度提高了几个数量级
- 神经元不被触发却在学习：在神经元和大脑连接处有更多电峰值：蛋白质，这些小生物机器让你的身体里的所有东西工作，结合局部电势进行大量的信息处理——不需要激活神经元
- 神经元动态改变其基因组以产生正确的蛋白质来进行日常信息处理任务。 大脑：“哦，你在看博客。等一下，我上调这个阅读基因来帮助你更好地理解博客的内容。”(这有点夸张，但也不算太离谱)

在我们研究大脑的复杂性之前，我们先来看看大脑模拟。大脑模拟通常被用来预测人类智智力。如果我们能模拟人脑，那么不久我们能够开发出类似人类的智慧，对吧？所以下一段看看这个推理。大脑模拟真的能提供可靠的证据来预测人工智能的出现吗?

### 大脑模拟的问题

大脑模拟模拟了神经元发出的电信号以及神经元之间连接的大小。大脑模拟从随机信号开始，整个系统依靠控制大脑中信息处理步骤的规则来稳定。在运行这些规则一段时间后，可以形成稳定的信号，与大脑的信号进行比较。如果模拟的信号与大脑的记录相似，这增加了我们对所选规则有些类似于大脑使用规则的信心。因此，我们可以验证大脑中大规模的信息处理规则。然而，大脑模拟的一个大问题是，这几乎是我们所能做的。

我们无法理解这些信号的含义或它们可能具有的功能。除了模糊的“我们的规则产生相似的活动”之外，我们无法用这个大脑模型来检验任何有意义的假设。缺乏精确的假设来做出准确的预测(“如果活动是这样的，那么电路检测到的是苹果而不是橘子”)是对欧洲大脑模拟项目[最大的批评之一](http://www.nature.com/news/neuroscience-where-is-the-brain-in-the-human-brain-project-1.15803)。大脑项目被许多神经科学家认为是[无用的](http://www.neurofuture.eu/) ，甚至是危险的，因为它会把钱浪费在有用的神经科学项目上，这些项目实际上为神经信息处理提供了线索。

另一个问题是这些大脑模拟依赖于过时、不完整并且这些模型在神经信息处理中忽略了许多生物部分。这主要是因为大脑中的电子信息处理更容易理解。 另一个更方便的原因是，当前模型已经能够重现所需的输出模式（毕竟这是主要目标），因此不需要更新这些模型以使其更像大脑。

总而言之，大脑模拟的问题是：

- 不可能测试具体的科学假设（将其与大型强子对撞机项目(large hadron collider project，简称lhc)的假设进行比较）
- 不能模拟真实的大脑处理(没有触发连接，没有生物相互作用)
- 没有深入了解大脑处理的功能（未评估模拟活动的意义）

最后一点是反对大脑处理对强AI评估有用的最重要的论据。如果我们能开发一个视觉系统的大脑模拟，这将在MNIST和ImageNet数据集上表现得更好，这将有助于估计大脑AI的进展。但是如果没有这些，或者没有任何类似的可观察功能，大脑模拟对于AI来说仍然是无用的。

根据这个说法，大脑模拟对于测试大脑中信息处理的假设一般规则仍然是有价值的——我们没有更好的办法—— 但它们对于理解大脑中的信息处理意味着什么是毫无用处的。 这就为AI的发展提供了不可靠的证据。 任何依靠大脑模拟作为预测未来强AI的证据都应该以极大的怀疑态度来看待。

### 估计大脑的计算复杂性

正如引言中所提到的，对大脑复杂性的估计已有十年之久，许多新发现使这一旧的估计过时了。 我从来没有看到过最新的估计，所以在这里我得出了自己的估计。 在此过程中，我将主要关注电化学信息处理并忽略神经元内的生物相互作用，因为它们太复杂了（这篇博客已经很长了）。 因此，这里得出的估计可以被认为是复杂性的下限 - 应该总是假设大脑比这更复杂。

在构建这种复杂模型的过程中，我还将模型中的每一步与其深度学习等价的东西联系起来。 这将使你更好地理解深度学习与人脑的紧密联系，人脑与深度学习相比有多快

### 定义模型的参考编号

我们知道一些事实和估计可以帮助我们开始构建模型：

- 大脑使用的学习算法与深度学习非常不同，但神经元的结构类似于卷积网络
- 成人大脑有860亿个神经元，大约10万亿个突触，大约3000亿个树突（树状结构上有突触）
- 儿童的大脑有1000多亿个神经元，突触和树突分别超过15万亿和1500亿
- 胎儿的大脑有超过一万亿的神经元;错位的神经元很快就会死亡(这也是为什么成年人的神经元比儿童少的原因)
- 小脑、大脑的超级计算机包含大约¾的神经元(这个比例在大多数哺乳动物物种中是一致的)
- 大脑是“智力”的主要驱动力,大约占所有神经元的¼
- 小脑中的一个普通神经元大约有25000个突触
- 大脑中的一个普通神经元大约有5000-15000个突触

[![小脑动画](https://i2.wp.com/timdettmers.com/wp-content/uploads/2015/07/cerebellum_animation_small.gif?resize=150%2C150)](https://i2.wp.com/timdettmers.com/wp-content/uploads/2015/07/cerebellum_animation_small.gif)

小脑的位置，其中包含大约3/4的所有神经元和连接。 图像来源: [1](https://commons.wikimedia.org/wiki/File:Cerebellum_animation_small.gif)

[![大脑动画](https://i0.wp.com/timdettmers.com/wp-content/uploads/2015/07/cerebrum_animation_small.gif?zoom=1.25&resize=150%2C150)](https://i0.wp.com/timdettmers.com/wp-content/uploads/2015/07/cerebrum_animation_small.gif)

大脑的位置，也被称为“皮质”。 更确切地说，皮质是大脑的外层，其包含大脑的大多数神经元。 图片来源：[1](https://commons.wikimedia.org/wiki/File:Cerebrum_animation_small.gif)

神经元的数量是已知的; 突触和树突的数量只有在一定的范围内才知道，我在这里保守估计一下。

每个神经元的平均突触在神经元之间差异很大，这里粗略计算了平均值。众所周知，小脑中的大多数突触是在浦肯野神经元的树突和两种不同类型的神经元之间形成的，这两种神经元与浦肯野的突触形成“攀爬”或“交叉平行”的连接。已知浦肯野细胞每个约有10万个突触。 因为这些细胞在小脑中具有迄今为止观察到的最大的重量，所以如果观察这些神经元及其产生的相互作用，我们就能最好地估计大脑的复杂性。

[![神经元类型](https://i0.wp.com/timdettmers.com/wp-content/uploads/2015/07/neuron_types.gif?zoom=1.25&resize=500%2C302)](https://i0.wp.com/timdettmers.com/wp-content/uploads/2015/07/neuron_types.gif)有数百种不同类型的神经元; 这里有一些比较常见的神经元。 感谢[Robert Stufflebeam](http://www.uno.edu/cola/philosophy/faculty/stufflebeam.aspx) 对于这个图像([来源](http://www.mind.ilstu.edu/curriculum/neurons_intro/neurons_intro.php)).

区分大脑区域的复杂性和功能重要性是很重要的。虽然几乎所有的计算都是由小脑完成，但几乎所有重要的功能都是由大脑(或皮层)完成。大脑皮层使用小脑做出预测、校正和下结论，但是皮层积累了这些见解并对它们起作用。

对于大脑来说，我们知道神经元具有的突触数量几乎从来没有超过50000个，并且与小脑不同的是，大多数神经元的突触数量都在5000-15000之间。

### 我们如何使用这些数字?

估计大脑计算复杂性的一种常用方法是假设大脑中的所有信息处理都可以由神经元发出脉冲（动作电位）和每个神经元突触大小（主要是受体数量）的组合来表示。 因此，可以将神经元数量及其突触的估计值相乘，并将所有数据相加。 然后将其乘以平均神经元的发射速度，即每秒约200个动作电位。 这个模型是Ray Kurzweil用来创建他的估计的模型。 虽然几十年前这种模型表现还可以，但从目前的观点来看，它并不适合对大脑进行建模，因为它遗漏了许多重要的神经信息处理，而这些信息处理远不止是激活神经元那么简单。

但是，这个扩展模型实际上与深度学习非常相似，因此我将在这里包含这些细节。
扩展的线性-非线性-泊松级联模型(LNP)可以更准确地模拟神经元的行为。扩展的LNP模型目前被看作是[神经元处理信息的精确模型](http://www.sciencedirect.com/science/article/pii/S0959438814000130)。然而，扩展的LNP模型仍然有一些细节问题，这些细节对于模拟大规模脑功能并不重要。实际上，将这些细节加入到模型中几乎不会增加额外的计算复杂度，但会使模型更复杂、难以理解。因此在模拟中包含这些细节会违反为定论找到最简单模型的科学方法。 然而，这个扩展模型实际上与深度学习非常相似，这些细节将包含在本文中。

还有其他好的模型也适用于此。我选择LNP模型的主要原因是它跟深度学习非常类似。我将在下一部分用这个模型比较神经元的结构与卷积网络的结构，同时我将得出对大脑复杂性的估计。

## 第二部分：大脑与深度学习 - 对比分析

现在我将逐步解释大脑是如何处理信息的。我将说明信息处理的步骤，这些步骤是很容易理解的，并且有可靠的证据支持。在这些步骤之上，在生物学层面（蛋白质和基因）有许多中间步骤，这些步骤仍未被充分理解，但已知对信息处理非常重要。我不会深入研究这些生物过程，而是提供一个简短的大纲，这可能会帮助渴望知识的读者自己深入研究。我们现在开始旅程，从一个发射神经元释放的神经递质，沿着它的所有过程走，直到到达下一个神经元释放它的神经递质的位置，这样我们就回到开始的地方。

下一节将介绍几个新术语，这些新术语是博客的其余部分所必需的，所以如果你不熟悉基本的神经生物学，请仔细阅读。

[![neuron_anatomy](https://i2.wp.com/timdettmers.com/wp-content/uploads/2015/07/neuron_anatomy1.jpg?zoom=1.25&resize=680%2C390)](https://i2.wp.com/timdettmers.com/wp-content/uploads/2015/07/neuron_anatomy1.jpg)图片来源: [1](https://commons.wikimedia.org/wiki/File:Neuron_Hand-tuned.svg),[2](https://commons.wikimedia.org/wiki/File:SynapseSchematic_lines.svg),[3](http://faculty.ivytech.edu/~shopper6/ANPweb/gallery/Week_011-2.html),[4](http://faculty.ivytech.edu/~shopper6/ANPweb/gallery/Week_011-2.html)

神经元利用轴突——一种管状结构——在大脑中长时间传输电信号。当一个神经元放电时，它会释放一个动作电位——沿着它的轴突分叉成一个小结尾的树，称为轴突末端。在每个轴突末端的每一个末端都有一些蛋白质将电子信息转化为化学信息:小球体——突触小泡——充满了一些神经递质，每个被释放到神经元外的区域，称为突触间隙。该区域将轴突末端与下一个神经元（突触）的开始分开，并允许神经递质自由移动以完成不同的任务。

突触通常位于一个看起来非常像树或植物的根的结构上，这是由树枝组成的树枝状树，树枝分枝成更大的臂（这代表神经网络中神经元之间的连接），最终到达细胞的核心，称为体细胞。这些树突几乎包含将一个神经元连接到下一个神经元的所有突触，从而形成主要连接。突触可以容纳数百种神经递质可以自身结合的受体。

您可以将这种轴突末端和突触的化合物想象成（卷积）输入层（图片）进入卷积网络。每个神经元可拥有少于5个树突或多达数十万个。之后我们将看到树突树的功能类似于卷积层的组合，随后是卷积网络中的池化层。

回到生物学过程，突触囊泡与轴突末端的表面融合，并从内到外将它们的神经递质溢出到突触间隙中。在那里，神经递质由于环境温度而振动发生漂移，直到它们（1）找到适合其键（神经递质）的合适的锁（受体蛋白），（2）神经递质遇到分解它们的蛋白质，或（3）神经递质遇到一种蛋白质，这种蛋白质将它们拉回轴突（再吸收），在那里它们被重复使用。抗抑郁药主要通过（3）预防或（4）促进神经递质5-羟色胺的再吸收的作用; （3）防止再吸收会在几天或几周后产生信息处理的变化，而（4）促进再吸收会导致在几秒或几分钟内发生变化。因此神经递质再吸收机制对于每分钟的信息处理是不可或缺的。在LNP模型中忽略了重新吸收的过程。

然而，神经递质释放的数量，给定神经递质的突触数量，以及实际上神经递质进入突触上拟合蛋白质的数量可以被认为是（全）连接层中的权重参数，以上都是神经网络中的一部分。换句话说，神经元的总输入是所有轴突 - 末端 - 神经递质 - 突触相互作用的总和。在数学上，我们可以将其等效为两个矩阵的点积（A点乘B; [所有输入的神经递质的数量]点乘[所有突触上拟合蛋白质的量]）。

在神经递质锁定到突触上的拟合蛋白后，它可以做很多不同的事：一般情况下，神经递质（1）打开通道，让带电粒子流入（通过扩散）进入树突，但它也会产生罕见的效果：神经递质（2）结合蛋白，然后产生蛋白质信号级联，（2a）激活（上调）一个基因，然后用于产生一种新的蛋白质。整合到神经元的表面，其树突和/或其突触中;（2b）通知现有蛋白质在特定位点发挥某种功能（产生或移除更多突触，开启一些入口，将新蛋白质附着到突触表面）。这是在NLP模型中被忽略的。

一旦通道打开，带负电或带正电的粒子进入树突棘。树突棘是一种小的蘑菇状结构，突触附着在上面。这些树突棘可以存储电势并具有自己的动态信息处理。这是在NLP模型中被忽略的。

[![dendritic_spine](https://i0.wp.com/timdettmers.com/wp-content/uploads/2015/07/dendritic_spine.jpg?zoom=1.25&resize=471%2C335)](https://i0.wp.com/timdettmers.com/wp-content/uploads/2015/07/dendritic_spine.jpg)树突棘具有其自身的内部信息处理动力学，其主要由其形状和大小决定。 图片来源: [1](https://en.wikipedia.org/wiki/File:Spline_types_3D.png),[2](https://en.wikipedia.org/wiki/File:Dendritic_spines.jpg)

进入树突棘的粒子带负电荷或带正电荷——打开通道的神经递质仅为负粒子，而剩下的为正粒子。还存在使带正电的粒子离开神经元的通道，从而增加电势的负面性（如果神经元变得过于阳性则会被“激发”）。蘑菇状树突棘的大小和形状与其行为相对应。这是在NLP模型中被忽略的。

一旦粒子进入脊柱，他们可以影响许多事。一般情况下，他们将（1）沿树突移动到神经元中的细胞体，然后，如果细胞过度带电（去极化），它们会诱发动作电位（神经元“发射”）。但是其他动作也很常见：带电粒子积聚在树突棘，并且（2）打开电压门控通道，这可以进一步使细胞极化（这是上面提到的树突脊柱信息处理的一个例子）。另一个非常重要的过程是（3）树突状尖峰。

### 树突状尖峰

树突状尖峰是一种已知存在多年的现象，但仅在2013年，这些技术已经足够先进，可以收集数据来显示这些尖峰对于信息处理来说非常重要。要测量树突峰值，必须在计算机的帮助下将一些非常小的夹子（clamps）连接到树突上，该计算机可以非常精确地移动夹具。为了了解夹具的位置，您需要一个特殊的显微镜来观察夹具，即便进入树枝状晶体时，夹子仍然需要长时间固定在一个相当盲目的物体（blind matter）上，因为在如此微小的范围内，世界上只有少数团队拥有将这种夹具连接到树突上的设备和技能。

但是，这些团队收集的直接数据足以将树突状尖峰作为重要的信息处理活动。由于将树突状尖峰引入神经元的计算模型中，单个神经元的复杂性变得非常类似于具有两个卷积层的卷积网络。正如我们后面将看到的，LNP模型也使用十分类似于非线性修正线性函数的功能，并且还使用与dropout非常相似的尖峰发生器（spike generator） - 因此神经元非常像整个卷积网络。但关于这一点的更多内容，需要回到讨论树突状尖峰及其究竟是什么。

当在树突中达到临界水平的去极化时，发生树突状尖峰。去极化放电作为一个电势沿着树突的墙壁去触发电压门控通道，如果电势足够强，那么电势就会达到神经元的核心，触发真正的动作点位。如果树突状尖峰未能触发动作电位，则在一瞬间内，相邻树突打开电压门控通道。由于从树突打开的通道，更多带电粒子进入神经元，然后可以触发（常见）或抑制（罕见）神经元细胞体（体细胞）的完整动作电位。

[![树突状尖峰](https://i0.wp.com/timdettmers.com/wp-content/uploads/2015/07/dendritic_spikes.png?zoom=1.25&resize=677%2C263)](https://i0.wp.com/timdettmers.com/wp-content/uploads/2015/07/dendritic_spikes.png)
A显示了不模拟树突状尖峰的神经元的计算机模型; B模拟树枝状尖峰的简单动态; C模拟树枝状尖峰的更复杂的动力学，其考虑了颗粒的一维扩散（类似于卷积操作）。请注意，这些图像只是特定时刻的快照。非常感谢[Berd Kuhn](https://groups.oist.jp/onu). 图片版权所有© 2014 Anwar, Roome, Nedelescu, Chen, Kuhn和De Schutter发表于*细胞神经科学前沿 （Anwar等人，2014）*

此过程与max-pooling非常相似，将单个大型激活“覆盖”其他相邻值。然而，在树突峰值之后，相邻值不会像在深度学习中使用的最大池化中那样被覆盖，但是电压门控通道的打开极大地放大了树突内所有相邻分支中的信号。因此，树枝状尖峰可以将相邻树突中的电化学水平提高到更接近最大输入的水平 - 该效果接近max-pooling。

实际上，已经证明视觉系统中的树突尖峰与用于物体识别的卷积网络中的max-pooling具有相同的目的：在深度学习中，最大池化用于实现（有限的）旋转，平移和尺度不变性（意味着我们的算法可以检测图像中的目标，其中目标被旋转，移动或缩小/放大几个像素）。可以将此过程视为将所有周围像素设置为相同的激活并使每个激活共享下一层的权重（在软件中，为了计算效率而舍去值 - 这在数学上是等效的）。类似地，已经表明视觉系统中的树突尖峰对物体的方向敏感。因此树突状峰值不仅具有计算相似性，而且还具有与之（方向敏感）相似的功能。

这个类比并没有结束。在神经网络反向传播期间——也就是当动作电位从细胞体传播回到树突时——信号不能反向传播到树突分支的源头，这是因为最近的电活动而被“停用”。因此，清晰（clear）的学习信号被发送到未激活的分支。一开始，这可能看起来与最大池的反向传播完全相反，除了最大池激活之外的所有内容都是反向传播的。然而，树突中没有反向传播信号是罕见的，并且树突本身代表学习信号。因此，产生树突状尖峰的树突具有特殊的学习信号，就像最大池中的激活单元一样。

为了更好地了解树枝状尖峰是什么以及它们看起来像什么，我非常希望你观看[这个视频](http://www.hhmi.org/research/how-do-neurons-compute-output-their-inputs) (我没有版权). 该视频显示了两个树突状尖峰如何导致动作电位。

树突和动作电位的结合以及树突树状结构被发现对海马体的学习和记忆至关重要，海马体是负责形成新记忆并在晚上将它们写入我们的“硬盘”的主要大脑区域。

树突峰值是计算复杂性的主要因素之一，这些因素是从大脑复杂性的过去模型中遗漏下来的。此外，这些新发现表明神经反向传播不一定是神经元到神经元来学习复杂的功能; 单个神经元已经实现了卷积网络，因此具有足够的计算复杂性来模拟复杂现象。因此，几乎不需要跨越多个神经元的学习规则——单个神经元也可以产生我们用卷积网络生成的相同输出。

但是这些发现树突峰值并不是唯一的进步在我们理解信息的处理步骤在这个阶段的神经信息处理途径
但是这些关于树突状尖峰的发现并不是我们理解信息处理步骤的这个阶段所取得的唯一进展。基因操作和蛋白质合成是将计算复杂性提高数量级的来源，直到最近取得了进展，揭示了生物信息处理的真正扩展。

### 蛋白质信号级联

正如我在本部分的介绍中所说，我不会广泛涉及生物信息处理的各个部分，但我想给你足够的信息，以便你可以从这里开始学习到更多。

必须要理解的一点是，细胞与教科书中的显示方式有很大不同。细胞爬行蛋白质：在任何给定的人类细胞中都有大约100亿个蛋白质，这些蛋白质并非空闲：它们与其他蛋白质结合，处理任务或移动以寻找新的任务。

上述所有功能都是蛋白质的。例如，锁定和锁定机制以及为离开和进入神经元的带电粒子起到看门人的通道作用都是蛋白质。我在本部分中所指的蛋白质不是这些常见蛋白质，而是具有特殊生物功能的蛋白质。

作为一个例子，丰富的神经递质谷氨酸可以与NDMA受体结合，然后NDMA受体为许多不同种类的带电粒子打开通道，并且在打开后，在神经元激发时，通道关闭。突触的强度十分依赖于该过程，其中突触根据NDMA受体的位置和反向传播到突触的信号并定时进行调整。我们知道这个过程对于大脑学习至关重要，但它只是大型工程中的一小部分。

可以进入神经元的带电粒子可以另外诱导蛋白质信号级联拥有它们自己的。例如，下面的级联显示了活化的NMDA受体（绿色）如何使带电的钙CA2 +在其内部触发级联，最终导致AMPAR受体（紫色）被搬运并安装在突触上。

[![观察AMPAR搬运](https://i0.wp.com/timdettmers.com/wp-content/uploads/2015/07/regulationofampartrafficking.jpg?zoom=1.25&resize=680%2C531)](https://i0.wp.com/timdettmers.com/wp-content/uploads/2015/07/regulationofampartrafficking.jpg)图片来源: [1](https://commons.wikimedia.org/wiki/File:RegulationOfAMPARTrafficking.jpg)

一次又一次地证明，这些特殊的蛋白质对神经元的信息处理有很大的影响，但很难从这个看似混乱的100亿蛋白质的群体中挑选出特定类型的蛋白质，并研究其准确的功能。研究结果通常很复杂，涉及许多不同蛋白质的反应链，直到达到所需的最终产品或最终功能。开始和结束功能通常是已知的，但不是从一个到另一个的确切路径。先进的技术有助于详细研究蛋白质，随着技术越来越好，我们将进一步了解神经元中的生物信息处理过程。

### 基因操作

生物信息处理的复杂性并不以蛋白质信号级联结束，100亿个蛋白质并不是完成其任务的工人的随机群体，但这些工人具有特定的数量，以满足目前相关的特定功能。所有这些都是由包含辅助蛋白，DNA和信使RNA（mRNA）的紧密反馈环控制的。

如果我们使用编程来比喻描述整个过程，那么DNA代表整个github网站及其所有公共包，而信使RNA是一个大型库，其中包含许多其其它具有不同功能的小型库（类似于C ++ boost库）。




这一切都始于想要解决的编程问题（检测到生物问题）。您可以使用谷歌和stackoverflow来找到可以用解决问题的库的建议，很快你会发现建议你使用库X来解决问题Y（在一个地方发现到了问题Y，根据已有的解决方案找到了蛋白质X细胞，蛋白质检测到这个缺陷然后级联成蛋白质信号的链，这导致该基因将可产生蛋白质X上调;这里的上调是一个“嘿！请生产更多吧！”信号到蛋白质X的细胞核所在的DNA）。你下载该库并进行编译（复制基因G（转录）短串mRNA构造一长串mRNA的DNA）。然后你用相应的配置进行配置安装（mRNA离开核心，mRNA转化为蛋白质，蛋白质可以在此之后通过其他蛋白质调整），并将库安装在全局“/lib”目录中（蛋白质折叠成正确的形状，之后它可以完全发挥作用）。安装完库后，将库中所需的部分导入您的程序（折叠的蛋白质（随机）移动到需要的位置）并使用该库的某些功能来解决您的问题（蛋白质的工作来解决这个问题）。

除此之外，神经元还可以动态地改变它们的基因组，也就是说它们可以动态地改变它们的github仓库来添加或删除库。

为了进一步了解这一过程，您可能需要观看以下视频，其中显示了HIV如何产生蛋白质以及病毒如何改变宿主DNA以满足其需要。此视频动画中描述的过程与神经元中发生的过程非常相似。为了使其与神经元中的过程更加相似，可以想象HIV是一种神经递质，并且HIV细胞中包含的所有物质首先都存在于神经元中。您所拥有的是准确表示神经元如何利用他们的基因和蛋白质：

[https://youtu.be/RO8MP3wMvqg](https://youtu.be/RO8MP3wMvqg)

您可能会问，是不是因为您体内的每个细胞都具有（几乎）相同的DNA以便能够自我复制？一般来说，大多数细胞都是这样，但大多数神经元不是这样。神经元通常具有与您在出生时分配的原始基因组不同的基因组。神经元可以具有额外或更少的染色体，并且从某些染色体中移除或添加信息序列。

结果表明，这种行为对于信息处理非常重要，如果出现问题，这可能会导致抑郁症或阿尔茨海默病等脑部疾病。最近还显示，神经元每天改变其基因组以改善信息处理需求。

因此，当你前五天坐在办公桌，然后在周末决定开始徒步旅行时，大脑会根据这项新任务调整其神经元，这是很有意义的，因为在环境变化后需要完全不同的信息处理。

同样，从进化的角度来看，在村庄内进行狩猎/采集和社交活动有不同的“模式”，这些是有益的——似乎这个功能适合这样的事。通常，生物信息处理设备在响应从几分钟到几小时的较慢信息处理需求方面非常有效。

关于深度学习，一个等效的功能是以重要也是基于规则的方式改变训练有素的卷积网络的功能; 例如，当从一个任务更改为另一个任务时，将变换应用于所有参数（识别街道数量->变换参数->识别行人）。

这种生物信息处理的任何内容都不是由LNP模型建立的。

回顾这一切，似乎很奇怪，许多研究人员认为他们只能通过专注于电化学特性和神经元间相互作用来复制大脑的行为。想象一下，卷积网络中的每个单元都有自己的github，从中*学习*并动态下载，编译和使用最好的库来解决某个任务。从这一切你可以看出，单个神经元可能比整个卷积网络更复杂，但我们继续从这里开始关注电化学过程，看看它在哪里引导我们。

### 回到LNP模型

在介绍完上面的这些之后，我们模型的信息处理只有一个相关的步骤。一旦达到临界水平的去极化，神经元通常会发射。但并非总是如此，也存在着阻止神经元发射的机制。例如，在神经元发射后不久，其电势太强而不能产生完全成熟的动作电位，因此它不能再次发射。即使在达到足够的电势时也可能存在这种阻塞，因为这种阻塞是生物功能而不是物理开关。

在LNP模型中，动作电位的这种阻塞是建模为具有泊松分布的非均匀泊松过程。以泊松分布为模型的泊松过程意味着神经元在第一次或第二次达到其阈值电位时具有非常高的发射概率，但也可能（以指数递减的概率）神经元可能不会发射多次。

[![泊松](https://i0.wp.com/timdettmers.com/wp-content/uploads/2015/07/poisson.png?zoom=1.25&resize=652%2C347)](https://i0.wp.com/timdettmers.com/wp-content/uploads/2015/07/poisson.png)具有随机抽取样本的泊松（0.5）分布。这里0,1,2,3表示神经元发射前的等待时间，因此0表示它会毫无延迟地发射，而2表示即使它可以物理发射也不会发射两个周期。

这个规则有例外，其中神经元禁用这种机制并以仅由物理学控制的速率连续发射——但这些是我将在此时忽略的特殊事件。通常，这整个过程与深度学习中使用的丢弃（dropout）非常相似，它使用均匀分布而不是泊松分布; 因此，这个过程可以看作是大脑使用的某种正则化方法而不是丢弃。

在下一步中，如果神经元发射，它会释放动作电位。动作电位的幅度差别很小，这意味着神经元产生的电位几乎总是具有相同的幅度，因此是可靠的信号。当这个信号沿着轴突传播时，它变得越来越弱。当它流入轴突末端的分支时，其最终强度将取决于这些分支的形状和长度; 因此每个轴突末端将接收不同量的电位。该空间信息与由于动作电位的尖峰模式引起的时间信息一起被转换成电化学信息（显示它们被转化为神经递质自身的峰值，持续约2ms）。调整输出信号,轴突末端可以移动,增加或减少(空间),或者它可能改变其蛋白质组成负责释放突触囊泡(时间)。

现在我们回到开始：神经递质从轴突末端释放（可以建模为稠密矩阵乘法）并且重复此步骤。

### 在大脑中的学习和记忆

现在我们已经完成了整个连续过程，让我们把所有这些都放到上下文中，看看大脑是如何协同使用这一切的。

大多数神经元每秒重复接收输入和发射过程约50至1000次; 射击频率高度依赖于神经元的类型以及神经元是否正在积极处理任务。即使神经元不处理任务，它也会以随机方式连续发射。一旦处理了一些有意义的信息，这种随机射击活动就会为大脑区域中相邻神经元之间的高度同步活动腾出空间。这种同步活动我们了解的不多，但被认为是理解大脑中信息处理及其学习方式不可或缺的一部分。

目前，尚不清楚大脑是如何学习的。我们知道它通过某种强化学习算法来调整突触以学习新的记忆，但是不清楚具体细节，而且少量矛盾的证据表明我们缺少一些重要的关键点。我们知道了全局，但是如果我们仍然缺乏的细节，我们无法弄清楚大脑的学习算法。

关于记忆，我们知道有些记忆直接存储在大脑的主要学习区域海马体中（如果你在每个大脑半球都失去了海马体，就无法形成新的记忆）。然而，大多数长期记忆是在您的REM睡眠阶段创建并与其他记忆整合的，当时所谓的睡眠轴将海马体的信息解除到所有其他大脑区域。长期记忆通常都是本地的：你的视觉记忆存储在视觉系统中; 你的舌头（味道，质地）的记忆存储在负责你舌头的大脑区域等等。

还已知海马体充当记忆缓冲液。一旦它充满，你需要睡觉以将其内容清空到大脑的其余部分（通过REM睡眠期间的睡眠轴）; 这可能就是为什么婴儿睡眠时间长，且不规律 - 他们的学习缓冲区已经满了，他们睡觉时要快速清理缓冲区，以便在醒来后学到更多东西。你仍然可以知道这个内存缓冲区是否已满，但保留时间更糟糕，新的内存可能会与缓冲区中的其他内存争夺空间并取代它们 - 这样可以真正获得所需的睡眠量。睡眠时间短、不规律是难以学习的，特别是对于需要学习的学生。

[![Hippocampus_small](https://i0.wp.com/timdettmers.com/wp-content/uploads/2015/07/hippocampus_small.gif?zoom=1.25&resize=200%2C200)](https://i0.wp.com/timdettmers.com/wp-content/uploads/2015/07/hippocampus_small.gif)每个半球的海马体显示为红色。图片来源: [1](https://commons.wikimedia.org/wiki/File:Hippocampus_small.gif)

因为在“写入缓冲区到硬盘驱动器”阶段，存储器与其他存储器集成在一起，所以睡眠对于创造力也非常重要。下次你在睡觉后回想起某段记忆时，它可能会被你的大脑认为适合附着在记忆中的一些新信息所改变。

我们可能都有过这个经历：我们醒来时有一些疯狂的新想法，只是看到它首先是非常荒谬的——所以我们的大脑也不完美并且犯错误。但其他时候它只是起作用：有一次我用数学问题不间断地折磨自己7个小时，只能失望地上床睡觉，只解决了大约四分之一的问题。在我醒来之后，我立即有两个新想法如何解决问题：第一个没有用; 但第二件事让事情变得非常简单，我可以在15分钟内勾勒出数学问题的解决方案 - 睡觉的颂歌！

现在，为什么我在谈论这个博客文章关于计算的回忆呢？事实上，内存创建——或者换句话说——一种长期存储计算结果的方法，对于任何智能都是至关重要的。在大脑模拟中，如果突触和激活发生在与真实大脑中相同的分布中，则会感到满意，但是人们并不关心这些突触或激活是否对应于任何有意义的事物——如功能所需的记忆或“分布式表示”，如对象识别。这是一个很大的缺陷。大脑模拟没有记忆。

在大脑模拟中，电化学粒子的扩散通过微分方程建模。这些微分方程是复杂的，但可以用欧拉方法等简单技术建模，以逼近这些复杂的微分方程。结果具有较差的准确性（意味着高误差），但该算法在计算上非常有效，并且精确度足以再现真实神经元的活动以及它们的大小和突触的分布。最大的缺点是我们通常无法从这样的方法中学习参数——无法创造有意义的记忆。

然而，正如我[写的关于卷积的博客文章](https://timdettmers.wordpress.com/2015/03/26/convolution-deep-learning/)，我们也可以通过应用卷积来模拟扩散——这是一种计算复杂的操作。卷积的优点是我们可以使用诸如最大似然估计和反向传播的方法来学习参数，这些参数导致类似于记忆的有意义的表示（就像我们在卷积网络中那样）。这与具有卷积运算的LNP模型完全类似。

因此，除了与深度学习模型的极大相似性之外，LNP模型也是合理的，因为它实际上可以学习产生有意义记忆的参数（其中，在这里的记忆是分布式表示的意思，就像我们在深度学习算法中找到的那样）。

这也证明了我通过在微分方程上使用卷积而不是欧拉方法来估计大脑复杂性的下一个点。

对于我们的模型而言，另一个要点是，我们目前没有为创建存储器分配复杂性（我们只模拟了前向传递，而不是后向传递和反向传播）。因此，我们低估了大脑的复杂性，但由于我们不知道大脑是如何学习的，因此我们无法对学习的计算复杂性做出任何准确的估计。有了这些并且保留在我们的脑海中，让我们继续将整个模型组合在一起，以获得计算复杂性的下限。

### 将它们整合在一起，以便对复杂性进行数学估算

[![brain_complexity](https://i2.wp.com/timdettmers.com/wp-content/uploads/2015/07/brain_complexity.png?zoom=1.25&resize=680%2C242)](https://i2.wp.com/timdettmers.com/wp-content/uploads/2015/07/brain_complexity.png)

下一部分有点棘手：我们需要估计N，M，n和m的数字，这些在神经元之间差别很大。

我们知道大脑中的860亿个神经元中有50个是小脑颗粒神经元，所以这些神经元及其连接在我们的估计中非常重要。

小脑颗粒神经元是非常微小的神经元，具有约4个树突。他们的主要输入来自大脑皮层。他们整合这些信号，然后沿着T形轴突发送它们，这些轴突进入Purkinje神经元的树突。

Purkinje神经元是迄今为止最复杂的神经元，但它们只有大约1亿个神经元。它们每个可能有超过100000个突触和大约1000个树突。多个Purkinje神经元将它们的输出捆绑在大约十二个深核（一堆密集的神经元）中，然后将信号发送回皮层。

这个过程对于非语言能力，抽象思维和抽象的创造力非常重要（创造力：写下多个以字母A开头的词;抽象的创造力：如果引力弯曲时空（广义相对论）怎么办？如果这些鸟属于当他们来到这个岛屿（进化）时，他们是同一个物种？）。几十年前人们认为，小脑只计算运动产出; 例如，当爱因斯坦的大脑被仔细处理和研究时，他的小脑基本上被切断并收起，因为它被认为是一个“原始的”大脑部分。

但从那时起，它表明小脑与皮质的大多数大脑区域形成1：1的连接。事实上，在23至25岁期间，小脑前部的变化可能会使您的非语言能力最多变为30分，并且一般在10-15个智商点上下浮动。这在大多数情况下非常有用，而我们失去了神经元执行一个功能，这些神经元执行我们在日常生活中不需要的功能（微积分，或者你学到但从未使用过的外语）。

因此，对小脑进行正确评估至关重要，不仅因为它包含大多数神经元，而且也是因为它对于重要情报和信息处理非常重要。
ral.

### 估计小脑过滤器尺寸

现在，如果我们看一个树枝，它会分支成几个分支，因此具有树状结构。沿着它的总长度通常包含突触。树枝状尖峰可以起源于枝晶的任何分支（空间维度）。当我们每个枝晶采取3个分支，总共4个树突时，我们有一个大小为3和4的卷积滤波器用于小脑颗粒神经元。由于两维上的线性卷积与一维上的卷积相同，然后是另一维上的卷积，我们也可以将其建模为单个3×4卷积运算。另请注意，这在数学上与描述源自不同来源（特征图）的粒子扩散的模型相同，后者根据其邻域（内核）中的规则进行扩散——这正是在物理层面发生的情况。[关于卷积的文章](https://timdettmers.wordpress.com/2015/03/26/convolution-deep-learning/).。

在这里，我选择用单个维度表示空间域。结果表明，树枝状树的形状在得到的信息处理中也很重要，因此我们需要空间域的两个维度。然而，数据缺乏以有意义的方式在数学上表示，因此我继续简化到一个空间维度。

时间维度在这里也很重要：带电粒子可能会停留一段时间，直到它们被泵出神经元。很难估计有意义的时间范围，因为大脑使用连续时间，而我们的深度学习算法只知道离散时间步长。

从生物学的角度来看，没有任何单一的估计是有意义的，但从心理学的角度来看，我们知道大脑可以在大约20毫秒内摄取图像中呈现的无意识信息（这只涉及大脑中一些快速，特殊的部分）。为了有意识地识别物体，我们需要更多的时间——至少65毫秒，平均约80-200毫秒，以获得可靠的意识识别。这涉及对象识别有效的所有常用部分。

根据这些估计，人们可以将此过程视为“在神经元内随着时间的推移建立所见图像的信息”。然而，如果神经元能够将有意义的信息与随机信息区分开来，神经元只能处理信息（请记住，如果神经元不主动处理信息，则会随机激发信息）。一旦存在某种程度的“有意义的信息”，神经元就会主动地对该信息作出反应。因此，在某种意义上，信息处理蔓延在大脑中的有用信息：如果相邻神经元已经感染了这些信息，则信息只能传播到一个神经元。以这种方式思考，这种信息流行在80-200毫秒内感染大脑中的所有神经元。

因此我们可以说，虽然对象在前20毫秒内缺少细节，但在大约80-200毫秒时有完整的细节。如果我们以每秒30帧（正常视频播放）——或者换句话说时间步长——将其转换为离散图像，那么20毫秒将是0.6个时间步长，并且80-200毫秒是2.4-6个时间步长。这意味着，神经元处理所需的所有视觉信息将在2.4至6帧内存在于神经元中。

为了使计算更容易，我现在在这里为神经过程选择5个时间步长的固定时间维度。这意味着对于树突，我们有小脑颗粒神经元大小为3x4x5的时空卷积滤波器。对于Purkinje神经元，类似的估计将是大约10x1000x5的过滤器。然后，非线性将这些输入减少为每个树突的单个数字。该数字表示瞬时点火率，即，该数字表示神经元在相应的时间间隔内发射的频率，例如5Hz，100Hz，0Hz等。如果电位负性过强，则不会产生尖峰（0 HZ）; 如果电位正性足够强，那么尖峰的大小通常与电位的大小成正比——但并非总是如此。

结果表明，该点火率的树突总和可以是线性的（总和），亚线性（小于和），超线性（大于和）或双稳态（小于和，或大于和） ，取决于各自的输入）; 这些求和行为通常不同于神经元到神经元。众所周知，Purkinje神经元使用线性求和，因此它们的求和形成尖峰率非常类似于深度学习中常用的整流线性函数max（0，x）。非线性总和可以被认为是不同的激活函数。重要的是，激活功能由神经元的类型决定。

可以将体细胞（或细胞体）中的滤波器视为空间域中大小为1的附加时间卷积滤波器。因此，这是一个过滤器，可以将输入减少到时间维度为5的单个维度，即1x1x5卷积过滤器（对于所有神经元，这将是相同的）。

同样，非线性然后将其降低到瞬时点火率，然后通过泊松过程将其丢弃，然后将其输入到权重矩阵中。

在这一点上，我想再次强调，将神经元的输出视为二进制是不正确的; 由射击神经元传达的信息更像是if-then-else分支：“if(fire == True and dropout == False){ release_ neurotransmitters(); }else{ sleep(0.02); }”

神经递质是神经元的真正输出，但这经常被混淆。这种混乱的根源在于，用突触研究神经递质释放及其动力学是非常困难的，而研究动作电位则非常容易。因此，大多数神经元模型将输出模拟为动作电位，因为我们在这里有很多可靠的数据; 我们没有实时水平的神经递质相互作用的数据。这就是为什么动作电位常常被混淆为神经元的真实输出。

当神经元发射时，这种冲动可以被认为是在轴突末端被转换成离散数（被释放的囊泡的数量）并且被另一个离散数乘以表示突触上的受体数量（这整个过程）对应于卷积网中的密集或完全连接的重量。在信息处理的下一步骤中，带电粒子进入神经元并建立实值电位。这与批量标准化（batch-normalization）也有一些相似之处，因为值被归一化到[0，阈值](neuron: relative to the initial potential of the neuron; convolutional net: relative to the mean of activations in batch-normalization)范围内（神经元：对应于于神经元的初始电位;卷积网：对应于批量标准化中的激活平均值）。当我们看整个过程,我们可以作为一个矩阵乘法模型两个实值矩阵(做一个按比例缩小的标准化之前或之后这是数学上是等价的，因为矩阵乘法是一个线性操作)。

因此，我们可以将神经元之间的轴突——末端——突触相互作用视为两个实值矩阵之间的矩阵乘法

### 估计小脑输入/输出维度

小脑颗粒神经元通常接收来自约四个轴突的输入（通常来自皮层的连接）。每个轴突与颗粒神经元的树突爪形成约3-4个突触（树突末端就好像你手里拿着一个网球一样），因此通过突触到颗粒神经元总共有大约15个输入。颗粒神经元本身以T形轴突结束，该轴突直接穿过Purkinje神经元的树突，与其形成约100个突触。

Purkinje神经元接收来自大约100000个与颗粒神经元连接的输入，并且它们自身在深核中形成大约1000个连接。据我所知，存在更高的估计值，并且没有准确的突触数量。100000个突触的数量可能略微过高（但75000会过于保守），但我仍然使用它，方便数学计算。

如上所述，所有这些维度都采用时间维度，因此例如颗粒神经元的输入具有15×5的维度。

因此，我们可以最终计算出小脑颗粒神经元与Purkinje神经元的复杂性。


[![brain_computational_estimate](https://i2.wp.com/timdettmers.com/wp-content/uploads/2015/07/brain_computational_estimate.png?zoom=1.25&resize=680%2C432)](https://i2.wp.com/timdettmers.com/wp-content/uploads/2015/07/brain_computational_estimate.png)

因此，对于大脑，我的估计将是1.075×10^21 FLOPS，截至2013年7月，地球上最快的计算机具有0.58×10^15 FLOPS用于实际应用（更多关于此内容）。

### 第三部分：限制和批评

虽然我讨论了大脑与深度学习的相似之处，但我没有讨论大脑是如何不同的。一个很大的差异是大脑中的dropout对所有输入起作用，而卷积网络中的dropout对每个单元起作用。大脑正在做什么对于现在的深度学习没有多大意义; 但是，如果你考虑将数百万个卷积网相互结合，那么像大脑那样做是很有意义的。大脑的dropout当然可以很好地解耦神经元的活动，因为没有神经元可以依赖来自单个其他神经元的信息（因为它可能被丢弃），所以它被迫考虑到所有与之相连的神经元，从而消除了偏差计算（基本上是正则化）。

该模型的另一个限制是它是一个下限。该估计未考虑：

- 反向传播，即从体细胞到树突的信号; 动作电位反映在轴突内并向后移动（这两件事可能几乎是复杂性的两倍）
- Axon末端信息处理
- 多神经递质囊泡（可以被认为是多个输出通道或过滤器，就像图像有多种颜色一样）
- 树突的几何形状
- 树突脊柱信息处理
- 非轴突突触（轴突—轴突和轴突—体细胞连接）
- 电突触
- 神经递质诱导蛋白质激活和信号传导
- 神经递质诱导基因调控
- 电压诱导（树突状尖峰和反向传播信号）基因调控
- 电压诱导蛋白激活和信号传导
- 胶质细胞（除了极度异常的大脑（大约十亿分之一），爱因斯坦也有异常高水平的神经胶质细胞）

所有这些都被证明对大脑中的信息处理很重要。我没有将它们包含在我的估算中，因为这样可以做到所有：

- 过于复杂：如果将其与生物信息处理的广泛性和复杂性进行比较，到目前为止我所讨论的内容非常简单
- 太特殊：非轴突突触可以具有与此处列出的所有内容完全不同的独特信息处理算法，例如相邻神经元束之间的直接电通信
- 和/或缺乏创建可靠数学模型的证据：神经反向传播，树枝状树木的几何形状和树突棘

请记住，这些估计值适用于整个大脑。当他们积极地处理刺激时，局部大脑区域可能具有比该平均值更高的计算处理速度。还要记住，小脑几乎构成了所有的计算处理。其他大脑区域整合了小脑的知识，但小脑充当了大脑中几乎所有信息（视觉和听觉除外）的转换和抽象模块。

### 但等等，但我们可以用更少的计算能力完成所有这些！我们已经拥有超人类的计算机视觉表现！

我不会说我们在计算机视觉方面有超人的表现。我们所拥有的是一个能够击败人类在系统中命名事物的系统，这些系统是从现实世界的背景中取出的（在我们看到现实世界中的某些东西突然形成我们的感知之前会发生什么）。我们几乎总能识别环境中的事物，但我们通常只是不知道（或关心）我们所看到的名称。

人类没有视觉系统来标记事物。尝试在现实世界中列出1000个常见物理对象——这不是一件容易的事。

为我们不承认一个对象，意味着我们看到一个对象但却无法理解它。如果你忘记了一位老同学的名字，那并不代表你不认识她; 这只是意味着你忘了她的名字。现在想象一下你从火车站出战，你知道一个好朋友正在站点的某个地方等你。你看到有人在300米远的地方挥舞着正朝你的方向看的手——这是你的朋友吗？不知道; 你不能认出是不是她。这就是纯粹的标签和物体识别之间的区别。

现在如果你无法识别30×30像素的图像，但计算机可以，这也不一定意味着计算机具有超人类物体识别性能。首先，这意味着你的视觉系统不适用于像素信息。我们的眼睛根本不习惯。

现在看看窗外，试着标记你看到的所有东西。这对大多数事物来说都很容易，但对于其他事物，你不知道正确的标签！例如，我不知道当我从窗户向外看时，我看到的几株植物的名称。但是，我们完全清楚我们所看到的是什么，并且可以列出该对象的许多细节。例如，通过评估它们的外观，我知道很多关于未知植物需要多少水和阳光，它们生长的速度，它们生长的方式，如果它们是老的或年轻的标本; 我知道如果我触摸它们会感觉如何——或者更一般地说——我知道这些植物如何在生物学上生长以及它们如何产生能量，等等。我不知道它的名字就可以做到这一切。目前的深度学习系统无法做到这一点，并且在相当长的一段时间内做不到。计算机视觉中的人性化表现确实很遥远！我们刚刚迈出了第一步（对象识别），现在的任务是使计算机视觉变得聪明，而不是使其擅长标记事物。

从进化的角度来说，我们的视觉系统的主要功能与命名我们看到的东西几乎没有关系：寻找和避免被猎杀，在觅食期间在自然界中定位自己，并确保我们选择正确的浆果并有效地提取根——这些都是重要的功能，但可能是我们愿景中最重要的功能之一是群体或关系中的社会功能。

如果你与某人Skype，那么当他们启用相机时，如果他们没有启用相机则会进行相当不同的通信。与亲自沟通相比，与静态2D表面上的图像进行通信也是非常不同的。视觉是沟通的关键。

我们的深度学习不能有效地做到这一点

### 理解没有标签的世界

一个值得注意的案例也证明了视觉的力量，无需任何标签就能真正理解环境，这就是[Genie](https://en.wikipedia.org/wiki/Genie_(feral_child))的情况。Genie被强制一个人待在房间里20个月。12年后，她被发现患有严重营养不良。在此期间她几乎没有社交互动，因此没有获得任何形式的口头语言。

一旦她与其他人接触，她就被教成英语作为一种语言（后来也是手语），但她从未真正掌握过它。相反，她很快掌握了非语言语言，并且在这方面表现非常出色。

对于陌生人，她几乎只与非语言的语言交流。有些情况下，这些陌生人会不顾其他，走到她身边，递给她一个玩具或其他东西——这个东西总是被称为喜欢和渴望的东西。

有一次，一名妇女在十字路口的一个红绿灯处下车，将她的钱包递给Genie。妇女和Genie没有说一句话; 他们完全非言语地相互理解。

所以Genie所做的就是通过她的视觉系统获取线索，并将该女性的情绪和认知状态转化为非语言提示和行为，然后她将用它来改变女性的心理状态。反过来，那个女人会想把钱包交给Genie（Genie可能甚至看不到）。

显然，Genie在非语言交流方面非常出色——但是如果你将她与深度学习对象识别系统联系起来会发生什么呢？在您选择的任何数据集上，深度学习系统将比Genie好得多。你认为卷积网在物体识别方面比Genie更好吗？我不这么认为。

这表明我们的计算机视觉方法是多么原始和天真。对象识别是人类视觉的一部分，但它并不是它的独特之处。

### 我们可以用更少的计算能力吗？

“我们不需要像大脑那样多的计算能力，因为我们的算法（将会）优于大脑。”

我希望你能在这篇博文中的描述后看到这句话是相当骄傲的。

我们不知道大脑是如何真正学习的。我们不了解大脑中的信息处理。但是，我们能说我们可以做得更好吗？

即使我们确实知道大脑的所有细节是如何运作的，但我们可以用更少的东西创造通用智能仍然是相当天真的想法。大脑在数亿年内通过进化发展起来。进化，它是最具可塑性的器官：人类大脑在过去的20000年中缩小了大约10％，人类的大脑迅速适应了我们使用口头语言的许多方式——这是进化术语中最近的一个发展。

还有研究表明，每只动物大脑中的神经元数量几乎等于它通过摄食所能维持的数量（我们可能在大约20000年前杀死了大部分猛犸象）。我们人类拥有如此大的大脑，因为我们发明了火和烹饪，我们可以通过它来消化食物，从而可以供给更多的神经元。没有烹饪，摄入的热量不足以供给我们的大脑而且我们会无助地饿死（至少在几千年前是这样;现在你可以轻松地吃纯素食——只需走进超市并买很多东西高卡路里的食物）。有了这个事实，很可能对大脑进行了足够的优化，以创造最佳的信息处理，这对于各种物种的典型卡路里摄入量是可能的——动物中最重要的功能将被最无情地优化以提高生存和生育。这也非常符合大脑的复杂性; 每个小功能都经过彻底优化，只有在技术进步的同时，我们才能逐步理解这种复杂性。

大脑中有数百种不同类型的神经元，每种神经元都有其指定的功能。实际上，神经科学家通常可以通过观察大脑区域中不断变化的结构和神经元类型来区分不同的大脑区域及其功能。虽然我们不了解电路如何执行信息处理的细节，但我们可以看到这些独特电路中的每部分都在精心设计以执行某种功能。这些回路通常在进化的不同物种中复制，这些物种具有共同的祖先，这些祖先在数亿年前分支到这些不同的物种中，表明这种结构在进化中对于它们正在处理的任务是最佳的。

深度学习中的等价物是，如果我们有10000个不同的卷积网络架构（具有自己的一组及以上激活函数），我们精心组合以改进算法的整体功能——你真的认为我们可以创造一些东西吗？可以产生复杂的信息处理，且遵循一个简单的通用架构？

当我们甚至无法理解其学习算法时，认为我们可以超越这个非常复杂的器官是相当天真的。

最重要的是，我们将开发比大脑更好的算法的说法是不可证实的。我们只有在实现它时才能证明它，我们不能反驳它。因此，这是一个相当荒谬的论述，几乎没有实际价值。即使没有足够的证据证明它们是正确的，理论通常也很有用。

标准物理模型是世界各地物理学家和工程师在日常生活中使用的极其有用的理论，用于开发我们喜欢的高科技产品; 然而这个理论并不完整，几天前它被修正，当时在LHC实验中证明了一个新粒子存在。

想象一下，如果有另一个模型，但只有当我们证明了所有粒子的存在时，你才能使用它。这种模型将毫无用处。当它根本没有对世界上的行为做出任何预测时，我们将无法用这种理论制造和开发电子产品。同样，我们可以开发比大脑更有效的算法的说法没有帮助; 相反，它使得进一步发展变得更加困难。大脑应该是我们的主要定位点。

另一个论证，对于Yann LeCun来说是典型的（他在一个小组中做了类似的论证）将是：可以说，飞机在飞行方面要比鸟类好得多; 然而，如果你描述鸟类的飞行，它是非常复杂的，每个细节都很重要，而飞机的飞行只是通过机翼周围的流体流动来描述。与大脑相比，为什么从深度学习中期待这种简单性是错误的？

我认为这个论点有一些道理，但基本上，它提出了错误的问题。我认为很明显，我们不需要为了实现人工智能而详细复制所有内容，但真正的问题是：我们的底线在哪里？如果你知道神经元可以用与卷积网非常相似的方式建模，那么你会说这个模型过于复杂而我们需要简化它吗？

## 第四部分：预测实际计算能力的增长

在高性能计算（HPC）中有一个主要的性能指标，这个指标是高性能LINPACK（HPL）基准测试中的每秒浮点运算（FLOPS）——它可以测量系统在一秒钟内可以执行的计算量。在数百或数千台计算机上进行分布式密集矩阵操作。在TOP 500超级计算机列表中，这是基于该基准的历史列表，该基准是新超级计算机系统性能的主要参考点。

但是一个大的LINPACK基准测试。[它并不反映](http://www.sandia.gov/~maherou/docs/HPCG-Benchmark.pdf)每天在现代超级计算机上运行的[实际应用中的性能](http://www.sandia.gov/~maherou/docs/HPCG-Benchmark.pdf)，因此，TOP 500列表中最快的计算机不一定是实际应用中最快的计算机。

高性能计算社区中的每个人都知道这一点，但它在这个领域的业务常规中根深蒂固，当你设计一个新的超级计算机系统时，你基本上必须证明你的系统能够在500强内，以获得该超级计算机的资金。

有时候这样的系统实际上是无法使用的，比如天河二号超级计算机，它在三年多的时间里仍然是LINPACK基准测试的首选。这台超级计算机的潜力大部分未被使用，因为它运行起来太昂贵（电力）而定制硬件（定制网络，Intel Xeon Phi）需要新软件，这需要多年的开发才能达到标准HPC软件的复杂程度。天河二号的运行速度仅为其容量的三分之一，换句话说，它几乎在3分钟内闲置了近2分钟。天河1号的前身是2010年世界上最快的计算机（根据LINPACK），由于官僚主义原因，自2013年以来一直没有使用过。

在中国以外，类似设计的其他超级计算机表现更好，但在实际应用中通常表现不佳。之所以如此，是因为像图形处理单元（GPU）或Intel Xeon这样的二手加速器可以在这样的设置中提供高FLOPS，但它们受到网络带宽瓶颈的严重限制。

为了纠正LINPACK基准测试不断增长的无用性，我们开发了一种新的性能指标：高性能共轭梯度基准测试（HPCG）。该基准测试执行共轭梯度，这需要比LINPACK更多的通信，因此更接近实际应用的性能数量。我将使用此基准来创建我对奇点的估计。

[![top500_2](https://i0.wp.com/timdettmers.com/wp-content/uploads/2015/07/top500_2.jpg?zoom=1.25&resize=680%2C544)](https://i0.wp.com/timdettmers.com/wp-content/uploads/2015/07/top500_2.jpg)过去十年的TOP500和HPCG的一些数据（数据收集最近才开始）。虚线表示预测。还展示了计算增长的主要驱动因素：多核CPU，GPU和2016-2017 3D存储器，以及2020年的一些新的未知技术。这种增长是否可持续？

然而，当我们假设这些应用程序基于深度学习时，这个基准测试仍然大大高估了人工智能应用程序可以达到的计算能力。

深度学习是目前最有前途的人工智能技术。可以肯定的是，深度学习——就像现在一样——是不够的，但可以肯定的是，与深度学习类似的东西将涉及到达强人工智能。

与其他应用程序不同，深度学习对网络带宽的需求异常高。它是如此之高，以至于对于TOP 500中的一些超级计算机设计而言，深度学习应用程序的运行速度比台式计算机慢。为什么会这样？因为并行深度学习涉及大规模参数同步，这需要大量的网络带宽：如果您的网络带宽太慢，那么在某些时候，深度学习越慢，您添加到系统的计算机就越多。因此，通常非常快的非常大的系统对于深度学习来说可能非常慢。

所有这些的问题在于，开发能够实现高带宽的新网络互连是困难的，并且进步比计算模块（如CPU，GPU和其他加速器）的进步要慢得多。就在最近，Mellanox达到了一个里程碑，他们可以制造出每秒100Gbps的交换机和InfiniBand卡。这种发展仍然是实验性的，并且难以制造能够以这种速度运行的光纤电缆。因此，到目前为止，还没有超级计算机实现这一新的开发。但随着这一具有里程碑意义的达成，很长一段时间内不会有另一个里程碑。网络互连带宽的倍增时间约为3年。

同样，存在内存问题。虽然CPU和GPU的理论处理能力的速度不断提高，但RAM的存储器带宽几乎不增长的。这是一个很大的问题，因为现在我们处于将数据移动到计算电路所花费的时间比实际使用它进行计算所花费的时间更多。

随着3D存储器等新发展，可以确保将实现内存带宽的进一步增加，但之后我们没有任何东西可以进一步提高性能。我们需要新的想法和新技术。内存不会通过越来越小而自我扩展。

然而，目前它们最大的障碍是功耗。天河二号使用24兆瓦的电力，每天电费为6.5万至10万美元，每年约为2300万美元。天河2号消耗的电力足以为德国的6000户家庭或美国的2000户家庭提供电力（空调用量）。

[![hpc_constraints](https://i0.wp.com/timdettmers.com/wp-content/uploads/2015/07/hpc_constraints.png?zoom=1.25&resize=680%2C430)](https://i0.wp.com/timdettmers.com/wp-content/uploads/2015/07/hpc_constraints.png)An 概述性能约束如何从旧的超级计算机变为新的超级计算机。改编自[霍斯特西蒙](http://www2.lbl.gov/Publications/Deputy-Director/bio.html)的[演讲](http://www.researchgate.net/profile/Horst_Simon/publication/261879110_Why_we_need_Exascale_and_why_we_won't_get_there_by_2020/links/0c960535dbade00bbc000000.pdf)

### 物理限制

此外，拐角处还有一个主要问题。很快，我们的电路将会很小，电子将开始显示量子效应。一种这样的量子效应是量子隧道效应。在量子隧穿中，电子同时位于两个相邻电路中，并随机决定接下来将进入这两个位置中的哪一个。

如果这种情况发生在宏观场景，就像在电视旁边给手机充电一样，电子决定他们想要用你的手机数据线而不是你的电视; 所以他们跳到电话线上切断了电视的电源。量子隧道将在2016-2017年开始相关，并且必须从那里开始考虑。需要新材料和“绝缘”电路，以便从这里开始工作。

使用新材料，我们需要新的生产技术，这将是非常昂贵的，因为所有的计算机芯片都依赖于一样的，老的但可靠的生产过程。我们需要进行研究和开发，以使我们已知的工艺与这些新材料一起使用，这不仅需要花钱，而且还需要花费时间。这也将推动一种持续的趋势，即生产计算机芯片的成本呈指数增长（并且增长可能因成本而放缓）。目前，这样的半导体制造工厂（晶圆厂）的价格为90亿美元，在过去几十年以相对稳定的速度增长，每年的成本增加约7-10％。

在此之后，我们处于普通的物理极限。晶体管将由不多于几个原子组成。我们不能小于此，这种制造水平需要大量努力才能使这些设备正常工作。这将在2025年左右开始，由于物理限制，增长可能会从这里开始放缓。

### 最近的计算能力增长趋势

总结前一节：（1）LINPACK性能不能反映实际性能，因为它不测试内存和网络带宽限制; （2）内存和网络带宽现在比计算能力更重要，但（3）内存和网络带宽的提升将是零星的，无法与计算能力的增长相抗衡; （4）电力成本是一个严重的限制（如果公民面临偶尔的停电，试图为超级计算机的专用电厂辩护），并且（5）计算能力将在未来几年内受到物理边界的限制。

因此，近年来计算能力的增长一直在放缓，这可能并不令人意外; 这主要是由于功率效率只会逐渐提高，但其他因素也会造成损失，例如无法跟上GPU等加速器的网络互连。

如果用当前估计的最快超级计算机的实际FLOPS，HPCG上具有0.58千万亿次浮点运算的天河-2，则需要21个倍增周期才能达到大脑计算能力的下限。如果使用摩尔定律，我们将在2037年达到这个定律; 如果我们采取过去60年的增长，即每个倍增期约为1.8年，我们将在2053年实现这一目标。如果由于上面列出的问题我们在倍增期间采用3年的较低估计，我们将在正常的超级计算应用中，内存带宽是目前实际应用的瓶颈，而这可能很快就会改变为网络带宽，每3年翻一番。所以2078年的估计可能非常准确。

[![growth](https://i2.wp.com/timdettmers.com/wp-content/uploads/2015/07/growth.jpg?zoom=1.25&resize=680%2C483)](https://i2.wp.com/timdettmers.com/wp-content/uploads/2015/07/growth.jpg)与HPCG基准相关的计算性能增长。假设计算性能和工厂成本分别以18或36个月的倍增期以指数速率稳步增长。

现在请记住，（1）HPCG基准测试比典型的深度学习应用程序具有更高的性能，这些应用程序更多地依赖于网络和内存带宽，以及（2）我对大脑计算复杂性的估计是下限。可以看出，2100年以后的估计可能并不太遥远。为了维持如此漫长而无情的计算性能提升，我们需要在2020年之前在物理限制边界运行时制定并实施许多新想法。这是否可行？

哪里有这样一种方式，有。 ——真正的问题是：我们是否准备投入资金？

# 结论

在这里，我讨论了大脑的信息处理步骤及其复杂性，并将它们与深度学习算法进行了比较。我专注于基本电化学信息处理和被忽视的生物信息处理的讨论。

我使用扩展的线性非线性Poisson级联模型作为基础工作并将其与卷积体系结构相关联。

通过这个模型，可以证明单个神经元具有与当前卷积网非常相似的信息处理结构，具有整流非线性的卷积级，然后通过类似dropout的方法对活动进行正则化。我还建立了最大池和电压门控通道之间的连接，这些通道由树枝状尖峰打开。存在与批量标准化的相似性。

这种直截了当的相似性使我们有充分的理由相信深度学习确实在正确的道路上。它还表明，从神经生物学过程中借鉴的思想对深度学习很有用（问题在于深度学习架构的进步往往先于神经生物学过程中的知识）。

我的模型显示，可以估计大脑每秒至少运行10x^21次操作。随着当前计算能力的增长速度，我们可以在2037年之前实现具有类似脑功能的超级计算机，但是当考虑所有证据时，2080年之后的估计似乎更为现实。如果我们成功地克服了物理障碍（例如量子隧道效应），半导体制造工厂的资本成本以及不断增长的电力成本等限制，那么这种估计就是正确的。与此同时，我们不断需要进行创新，以解决内存带宽和网络带宽问题，这些问题已成为或将成为超级计算的瓶颈。考虑到这些因素，实际上我们不太可能很快实现类似人类的处理能力。

## 结束语

我在这篇博文中的理念是在一个网页上展示所有信息，而不是散布信息。我认为这种设计有助于创造一种更加坚固的知识结构，通过其不同领域的交织，有助于更全面地了解所涉及的主要思想。然而，将所有这些信息组织成一个连贯的图片是非常困难的，有些观点可能比启发更令人困惑。请在下面留下评论，让我知道结构和内容是否需要改进，以便我可以相应调整我的下一篇博文。

我也很喜欢这篇博文的一般反馈。

另外，请务必与深度学习的同事分享此博客文章。具有原始计算机科学背景的人经常怀疑对大脑，其部分及其如何工作的误解。我认为这篇博文可能是一个合适的补救措施。

## 下一篇博文

本系列关于神经科学和心理学的第二篇文章将关注最重要的大脑区域及其功能和连接性。本系列的最后一部分和第三部分将重点关注心理过程，例如记忆和学习，以及我们可以从深度学习中学到的东西。

#### **致谢**

感谢Alexander Tonn的有效建议和校对这篇博文。

#### **重要参考资料和来源**

**神经科学**

Brunel, N., Hakim, V., & Richardson, M. J. (2014). Single neuron dynamics and computation. *Current opinion in neurobiology*, *25*, 149-155.

Chadderton, P., Margrie, T. W., & Häusser, M. (2004). Integration of quanta in cerebellar granule cells during sensory processing. *Nature*, *428*(6985), 856-860.

De Gennaro, L., & Ferrara, M. (2003). Sleep spindles: an overview. *Sleep medicine reviews*, *7*(5), 423-440.

Ji, D., & Wilson, M. A. (2007). Coordinated memory replay in the visual cortex and hippocampus during sleep. *Nature neuroscience*, *10*(1), 100-107.

Liaw, J. S., & Berger, T. W. (1999). Dynamic synapse: Harnessing the computing power of synaptic dynamics. *Neurocomputing*, *26*, 199-206.

Ramsden, S., Richardson, F. M., Josse, G., Thomas, M. S., Ellis, C., Shakeshaft, C., … & Price, C. J. (2011). Verbal and non-verbal intelligence changes in the teenage brain. *Nature*, *479*(7371), 113-116.

Smith, S. L., Smith, I. T., Branco, T., & Häusser, M. (2013). Dendritic spikes enhance stimulus selectivity in cortical neurons in vivo. *Nature*, *503*(7474), 115-120.

[Stoodley, C. J., & Schmahmann, J. D. (2009). Functional topography in the human cerebellum: a meta-analysis of neuroimaging studies. *Neuroimage*,*44*(2), 489-501.](http://pnns.org/pdf/STOODLEY%20and%20SCHMAHMANN%20Cerebellum%20Meta-analysis%20functional%20topography%20NeuroImage%202008.pdf)

**高性能计算**

Dongarra, J., & Heroux, M. A. (2013). Toward a new metric for ranking high performance computing systems. *Sandia Report, SAND2013-4744*, *312*.

[PDF: HPCG Specification](https://software.sandia.gov/hpcg/doc/HPCG-Specification.pdf)

[Interview: Why there will be no exascale computing before 2020](http://www.top500.org/blog/no-exascale-for-you-an-interview-with-berkeley-labs-horst-simon/)

[Slides: Why there will be no exascale computing before 2020](http://www.researchgate.net/profile/Horst_Simon/publication/261879110_Why_we_need_Exascale_and_why_we_won't_get_there_by_2020/links/0c960535dbade00bbc000000.pdf)

[Interview: Challenges of exascale computing](http://www.vrworld.com/2015/03/23/jack-dongarra-on-the-great-exascale-challenge-and-rising-hpc-powers/)

**图片参考**

[Anwar, H., Roome, C. J., Nedelescu, H., Chen, W., Kuhn, B., & De Schutter, E. (2014). Dendritic diameters affect the spatial variability of intracellular calcium dynamics in computer models. *Frontiers in cellular neuroscience*, *8*.](http://www.ncbi.nlm.nih.gov/pmc/articles/PMC4107854/)
